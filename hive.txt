Project Title: Data Pipeline and Remote Development using Google Cloud Platform (GCP) and IntelliJDescription: Developed a robust data pipeline on Google Cloud Platform (GCP) utilizing Dataproc, HDFS, Hadoop, Spark, and IntelliJ for remote development. The project focuses on automating data ingestion, processing, and storage in a Hive Parquet table through a scheduled ETL (Extract, Transform, Load) process for a dataset of thousands of rows, with comprehensive logging of Hive applications.Key Responsibilities:Environment Setup:Configured Google Cloud Platform (GCP) to set up a scalable and secure data processing environment.Deployed Google Cloud Dataproc clusters to manage and process large datasets using Hadoop and Spark.Data Ingestion and Storage:Implemented a data ingestion pipeline using SQL scripts to load a dataset of thousands of rows into Hadoop Distributed File System (HDFS).Scheduled data loading processes using a Cron scheduler for automated and timely execution.Data Processing and Transformation:Developed HQL (Hive Query Language) scripts to transform and process raw data stored in HDFS using Hive.Leveraged Spark for distributed data processing, ensuring efficient handling of the dataset.Ensured data quality and consistency through comprehensive validation and transformation logic.Data Storage and Management:Loaded processed data into Hive tables using HQL scripts.Utilized Hive's capabilities to store data in Parquet format, optimizing storage and query performance.Automation and Scheduling:Configured Cron jobs to automate the execution of SQL and HQL scripts, ensuring regular data updates.Monitored and maintained the data pipeline to ensure seamless and efficient data processing.Development and Debugging:Used IntelliJ IDEA for remote development, facilitating coding, debugging, and version control.Logging and Monitoring:Implemented comprehensive logging for Hive applications, ensuring visibility into data processing workflows.Monitored logs to detect and troubleshoot issues promptly, maintaining data pipeline reliability.Performance Optimization:Optimized data processing workflows for performance, leveraging Hadoop and Spark to ensure efficient resource utilization.Implemented best practices for data storage and retrieval, reducing query execution times.Technologies and Tools:Google Cloud Platform (GCP)Google Cloud DataprocHadoop Distributed File System (HDFS)HadoopSparkHive and HQL (Hive Query Language)Parquet (Columnar Storage Format)SQLCron SchedulerIntelliJ IDEA (Integrated Development Environment)Logging and Monitoring ToolsImpact:Streamlined the data ingestion and processing pipeline, resulting in a 30% reduction in data processing time.Improved data accessibility and performance with the use of Hive and Parquet, enabling faster query responses.Automated data workflows, reducing manual intervention and ensuring timely data updates.Enhanced visibility and troubleshooting capabilities through comprehensive logging of Hive applications.